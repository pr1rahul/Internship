{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4da979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc458e14",
   "metadata": {},
   "source": [
    "1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df69f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscrap(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    movies_name = soup.find_all('div', class_='lister-item-content')\n",
    "    \n",
    "    name = []\n",
    "    rating = []\n",
    "    year_of_release = []\n",
    "    \n",
    "    for movie in movies_name:\n",
    "        # movie name extracting\n",
    "        m_name= movie.find('a').text.strip()\n",
    "        name.append(m_name)\n",
    "        \n",
    "        # rating extracting\n",
    "        m_rating = movie.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "        rating.append(m_rating)\n",
    "        \n",
    "        #year of release extracting\n",
    "        m_yor = movie.find('span',class_=\"lister-item-year\").text.strip('()')\n",
    "        year_of_release.append(m_yor)\n",
    "        \n",
    "    df = pd.DataFrame({\"Name\":name, \"Rating\":rating, \"Year of Release\":year_of_release})\n",
    "    return df\n",
    "url= \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "webscrap(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0eabe6",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\" https://www.patreon.com/coreyms\")\n",
    "soup = BeautifulSoup(page.text)\n",
    "heading = []\n",
    "for i in soup.find_all('div', class_=\"sc-bBHxTw iloeMK\"):\n",
    "    heading.append(i.text.strip())\n",
    "\n",
    "date=[]\n",
    "for i in soup.find_all(\"span\", class_=\"sc-lgu5zg-0 dXpjXs\"):\n",
    "    date.append(i.text.strip())\n",
    "    \n",
    "content=[]\n",
    "for i in soup.find_all('div', class_=\"sc-1cq1psq-0 kWkNqn sc-pn81yk-0 dBfLIG\"):\n",
    "    content.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c917a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "h = soup.find('div', class_=\"sc-bBHxTw iloeMK\")\n",
    "h\n",
    "heading = []\n",
    "for i in soup.find_all('div', class_=\"sc-bBHxTw iloeMK\"):\n",
    "    heading.append(i.text)\n",
    "date = []\n",
    "content = []\n",
    "likes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cdc6f",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb380de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page= requests.get(\"https://www.nobroker.in/property/rent/mumbai/multiple?searchParam=W3sibGF0IjoxOS4xMzMzMTY1LCJsb24iOjcyLjg2NDg5NTgsInBsYWNlSWQiOiJDaElKd3l3ZFZOaTM1enNSMS1SLWp4VmZnSnciLCJwbGFjZU5hbWUiOiJJbmRpcmEgTmFnYXIifSx7ImxhdCI6MTkuMjIxNzg2OCwibG9uIjo3Mi44NjAwMjcxLCJwbGFjZUlkIjoiRWtwS1lYbGhJRTVoWjJGeUlGSmtMQ0JTWVdwbGJtUnlZU0JPWVdkaGNpd2dRbTl5YVhaaGJHa3NJRTExYldKaGFTd2dUV0ZvWVhKaGMyaDBjbUVnTkRBd01EWTJMQ0JKYm1ScFlTSXVLaXdLRkFvU0NYMVRUU3ZUc09jN0VXMEsxQWJUMHdpdUVoUUtFZ25OVk1WcDA3RG5PeEV3c0F1YWgxenNQQSIsInBsYWNlTmFtZSI6IkpheWEgTmFnYXIgUm9hZCJ9LHsibGF0IjoxOC45ODExNjI5LCJsb24iOjczLjA2NTk3OTA5OTk5OTk5LCJwbGFjZUlkIjoiRWpsU1lXcGhhbWtnVUdGMGFDd2dWV3gzWlN3Z1RtRjJhU0JOZFcxaVlXa3NJRTFoYUdGeVlYTm9kSEpoSURReE1ESXdOaXdnU1c1a2FXRWlMaW9zQ2hRS0VnbXRFZWRkdnNMbk94R1owZVVKeDBCVFpSSVVDaElKQ1JwYzR0X0M1enNSZ3doamlOWEVUMGciLCJwbGFjZU5hbWUiOiJSYWphamkgUGF0aCJ9XQ==&radius=2.0&sharedAccomodation=0&city=mumbai&locality=Indira%20Nagar,Jaya%20Nagar%20Road,Rajaji%20Path&nbFr=New_HomePage\")\n",
    "soup= BeautifulSoup(page.content)\n",
    "\n",
    "# Extract house title\n",
    "title = []\n",
    "for i in soup.find_all('h2',class_=\"overflow-hidden overflow-ellipsis whitespace-nowrap max-w-80pe po:max-w-full\"):\n",
    "    title.append(i.text.strip())\n",
    "    \n",
    "#Extract location \n",
    "locs =[]\n",
    "for i in soup.find_all(\"span\", class_=\"overflow-hidden overflow-ellipsis whitespace-nowrap text-13 text-gray-lightest\"):\n",
    "    locs.append(i.text)\n",
    "    \n",
    "# Extract Area\n",
    "area =[]\n",
    "for i in soup.find_all(\"span\", class_=\"flex justify-center\"):\n",
    "    area.append(i.text)\n",
    "\n",
    "#Extract EMI\n",
    "Not found\n",
    "\n",
    "#Expract price\n",
    "price =[]\n",
    "for i in soup.find_all(\"span\", class_=\"text-18 font-bold leading-22p\"):\n",
    "    price.append(i.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc0aca",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/bestseller?sort=popular .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bafc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "page =requests.get(\" https://www.bewakoof.com/bestseller?sort=popular\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "# Extract product name\n",
    "name = []\n",
    "for i in soup.find_all('h2', class_=\"clr-shade4.h3-p-name.undefined false\"):\n",
    "    name.append(i.text.strip())\n",
    "\n",
    "# Extract Price\n",
    "price = []\n",
    "for i in soup.find_all('div', class_=\"discountedPriceText clr-p-black false\"):\n",
    "    price.append(i.text.strip())\n",
    "    \n",
    "#Extract Image Url\n",
    "\n",
    "images = []\n",
    "for i in soup.find_all(\"img\", class_=\"productImgTag\" ):\n",
    "    images.append(i['src'])\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715a538",
   "metadata": {},
   "source": [
    "7) Please visit https://www.cnbc.com/world/?region=world and scrap- a) headings b) date c) News link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "# Extract Headings\n",
    "headings = []\n",
    "for i in soup.find_all('h2'):\n",
    "    headings.append(i.text.strip())\n",
    "headings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract date\n",
    "date=[]\n",
    "for i in soup.find_all('span', class_=\"MarketCard-lastTime\"):\n",
    "    date.append(i.text.strip())\n",
    "    \n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ebdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract news link\n",
    "#news_items = soup.find_all('div', class_='Card-titleContainer')\n",
    "news_link = soup.find('a', class_='LatestNews-headline').get('href')\n",
    "\n",
    "    \n",
    "print(f\"News Link: {news_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453f697",
   "metadata": {},
   "source": [
    "8) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded- articles/ and scrap- a) Paper title b) date c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "# extract Paper title\n",
    "paper_title = []\n",
    "for i in soup.find_all('h2', class_=\"h5 article-title\"):\n",
    "    paper_title.append(i.text.strip())\n",
    "paper_title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7021859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date\n",
    "date = []\n",
    "for i in soup.find_all('p',class_=\"article-date\"):\n",
    "    date.append(i.text.strip())\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Author\n",
    "author = []\n",
    "for i in soup.find_all('p', class_=\"article-authors\"):\n",
    "    author.append(i.text.strip())\n",
    "author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649632e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
