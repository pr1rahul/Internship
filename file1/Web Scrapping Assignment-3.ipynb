{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cdf236e",
   "metadata": {},
   "source": [
    "# Web Scrapping Assignment-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d680fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Necessary Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256faab9",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web browser\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web page\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take input from user and search for that product\n",
    "user_ip = str(input(\"Enter product to search: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for user input product\n",
    "product = driver.find_element(By.ID,\"twotabsearchtextbox\")\n",
    "product.send_keys(user_ip)\n",
    "product.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on search\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8da21",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all product urls\n",
    "product_urls=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start, end): # loop for scraping 3 pages \n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in url:\n",
    "        product_urls.append(i.get_attribute(\"href\")) # scraping url in product_urls list\n",
    "   \n",
    "    time.sleep(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb11177",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name=[]\n",
    "product_name=[]\n",
    "price=[]\n",
    "ret_exch=[]\n",
    "expected_delivery=[]\n",
    "availability=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in product_urls: # loop for every guitar in the list\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # brand name\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[5]/div[3]/div[4]/div[49]/div/table/tbody/tr[1]/td[2]/span')\n",
    "        brand_name.append(brand.text)\n",
    "    except:\n",
    "        brand_name.append(\"-\")\n",
    "        \n",
    "    # product name\n",
    "    try:\n",
    "        product=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[5]/div[3]/div[4]/div[1]/div/h1/span')\n",
    "        product_name.append(product.text)\n",
    "    except:\n",
    "        product_name.append(\"-\")\n",
    "        \n",
    "    #price\n",
    "    try:\n",
    "        price_list=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[5]/div[3]/div[4]/div[14]/div/div/div[4]/div[1]/span[3]/span[2]/span[2]')\n",
    "        price.append(price_list.text)\n",
    "    except:\n",
    "        price.append(\"-\")\n",
    "        \n",
    "    # Return/exchange\n",
    "    try:\n",
    "        ret_exchange=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[5]/div[3]/div[4]/div[25]/div[2]/div/div/div/div[2]/div/ol/li[3]/div/span/div[2]/span')\n",
    "        ret_exch.append(ret_exchange.text)\n",
    "    except:\n",
    "        ret_exch.append(\"-\")\n",
    "        \n",
    "    # expected delivery\n",
    "    try:\n",
    "        expected_del=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[5]/div[3]/div[1]/div[4]/div/div[1]/div/div/div/form/div/div/div/div/div[4]/div/div[3]/div[10]/div[1]/div/div/div[1]/span/span')\n",
    "        expected_delivery.append(expected_del.text)\n",
    "    except:\n",
    "        expected_delivery.append(\"-\")\n",
    "        \n",
    "    # Availability\n",
    "    try:\n",
    "        avail=driver.find_element(By.XPATH, '/html/body/div[2]/div/div[5]/div[3]/div[1]/div[4]/div/div[1]/div/div/div/form/div/div/div/div/div[4]/div/div[5]/div/div[1]/span')\n",
    "        availability.append(avail.text)\n",
    "    except:\n",
    "        availability.append(\"-\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Brand Name\":brand_name, \"Name of the Product\": product_name, \"Price\": price, \"Return/Exchange\": ret_exch, \"Expected Delivery\": expected_delivery, \"Availability\": availability, \"Product URL\":product_urls})\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eeee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc231ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6b9183f",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web browser\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac911497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web page\n",
    "driver.get(\"https://images.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search fruit\n",
    "product=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "product.send_keys(\"Cakes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8039719",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element(By.CLASS_NAME,\"Tg7LZd\")\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to scroll page to load images\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "    \n",
    "image_urls=[]\n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"YQ4gaf\"]')\n",
    "for image in images:\n",
    "    source=image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if (source[0:4]=='http'):\n",
    "            image_urls.append(source)\n",
    "for i in range(len(image_urls)):\n",
    "    if i >10:\n",
    "        break \n",
    "    response=requests.get(image_urls[i])\n",
    "    file=open(r\"C:\\Users\\rahul\\OneDrive\\Desktop\\data trained\\internship\\web scraping\\Selenium\\assignment q3\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2372a",
   "metadata": {},
   "source": [
    " 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web browser\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web page\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search \n",
    "product=driver.find_element(By.CLASS_NAME,\"Pke_EE\")\n",
    "product.send_keys(\"Oneplus Nord\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da23c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element(By.CLASS_NAME,\"_2iLD__\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ab0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all product urls\n",
    "product_urls=[]\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end): # loop for scraping first pages \n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"CGtC98\"]')\n",
    "    for i in url:\n",
    "        product_urls.append(i.get_attribute(\"href\")) # scraping url in product_urls list\n",
    "   \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name=[]\n",
    "smartphone_name=[]\n",
    "color=[]\n",
    "ram=[]\n",
    "storage=[]\n",
    "pri_cam=[]\n",
    "sec_cam=[]\n",
    "disp_size=[]\n",
    "bat_cap=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in product_urls: # loop for every smartphone in the list\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # brand name\n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[4]/div/div[1]/div/img')\n",
    "        brand_name.append(brand.text)\n",
    "    except:\n",
    "        brand_name.append(\"-\")\n",
    "        \n",
    "    # smartphone name\n",
    "    try:\n",
    "        smartphone=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[1]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        smartphone_name.append(smartphone.text)\n",
    "    except:\n",
    "        smartphone_name.append(\"-\")\n",
    "        \n",
    "       # color\n",
    "    try:\n",
    "        colr=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[5]/div/div[1]/div[1]/span')\n",
    "        color.append(colr.text)\n",
    "    except:\n",
    "        color.append(\"-\")\n",
    "        \n",
    "    # RAM\n",
    "    try:\n",
    "        rm=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[4]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        ram.append(rm.text)\n",
    "    except:\n",
    "        ram.append(\"-\")\n",
    "        \n",
    "     # Storage\n",
    "    try:\n",
    "        strg=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        storage.append(strg.text)\n",
    "    except:\n",
    "        storage.append(\"-\")\n",
    "        \n",
    "    # primary camera\n",
    "    try:\n",
    "        pr_cm=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[7]/div[1]/div/div[2]/ul/li[3]')\n",
    "        pri_cam.append(pr_cm.text)\n",
    "    except:\n",
    "        pri_cam.append(\"-\")\n",
    "        \n",
    "    #secondary camera\n",
    "    try:\n",
    "        sc_cm=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[7]/div[1]/div/div[2]/ul/li[3]/1')\n",
    "        sec_cam.append(sc_cm.text)\n",
    "    except:\n",
    "        sec_cam.append(\"-\")\n",
    "        \n",
    "    #display size\n",
    "    try:\n",
    "        display=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        disp_size.append(display.text)\n",
    "    except:\n",
    "        disp_size.append(\"-\")\n",
    "        \n",
    "     # battery capacity\n",
    "    try:\n",
    "        battery=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[7]/div[1]/div/div[2]/ul/li[4]')\n",
    "        bat_cap.append(battery.text)\n",
    "    except:\n",
    "        bat_cap.append(\"-\")\n",
    "        \n",
    "    # price\n",
    "    try:\n",
    "        prc=driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[1]/div[2]/div[2]/div/div[4]/div[1]/div/div[1]')\n",
    "        price.append(prc.text)\n",
    "    except:\n",
    "        price.append(\"-\")\n",
    "  \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cefd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Brand Name\":brand_name, \"Smartphone name\":smartphone_name, \"Colour\":color, \"RAM\":ram, \"Storage(ROM)\":storage, \"Primary Camera\": pri_cam, \"Secondary Camera\":sec_cam, \"Display Size\":disp_size, \"Battery Capacity\":bat_cap, \"Price\": price, \"Product URL\": product_urls})\n",
    "df.to_csv(\"outputq4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36248157",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web browser\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening web page\n",
    "driver.get(\"https://www.google.com/maps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search fruit\n",
    "product=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[8]/div[3]/div[1]/div[1]/div/div[2]/form/input\")\n",
    "product.send_keys(\"Amravati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6523af",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[8]/div[3]/div[1]/div[1]/div/div[2]/div[1]/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_url='https://www.google.com/maps/place/Amravati,+Maharashtra/@20.9040872,77.7617455,11z/data=!3m1!4b1!4m6!3m5!1s0x3bd6a4a67774bc15:0x3c7b3f78ca4f9635!8m2!3d20.9319821!4d77.7523039!16zL20vMDM3dm1y?entry=ttu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150386c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    url_string=driver.current_url\n",
    "    print(\"URL EXTRACTED: \", url_string)\n",
    "    lat_lng=re.findall(r'@(.*)data',url_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e31ed587",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112bb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d62abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c91e67",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5c9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de87bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a211b4",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e38206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa3222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26f4d011",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce5c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
